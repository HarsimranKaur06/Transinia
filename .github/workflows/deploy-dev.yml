name: CI/CD â€” build, push to ECR, deploy to ECS (dev)

on:
  push:
    branches: [ dev ]

env:
  ENVIRONMENT: dev
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  ECR_REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com
  BACKEND_ECR_REPO: transinia-dev-backend
  FRONTEND_ECR_REPO: transinia-dev-frontend
  CLUSTER_NAME: transinia-dev-cluster
  BACKEND_SERVICE: transinia-dev-backend-service
  FRONTEND_SERVICE: transinia-dev-frontend-service
  BACKEND_API_URL: http://transinia-dev-alb-1892657611.us-east-1.elb.amazonaws.com

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
    # Add always() to ensure cleanup runs even if job fails
    if: always()

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Create ECR repositories if missing
        run: |
          aws ecr describe-repositories --repository-names $BACKEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $BACKEND_ECR_REPO
          aws ecr describe-repositories --repository-names $FRONTEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $FRONTEND_ECR_REPO

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v1
        id: ecr-login
        with:
          mask-password: 'true'

      # Set up Terraform
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.5

      # Deploy DynamoDB infrastructure
      - name: Deploy DynamoDB Infrastructure
        working-directory: ./backend/infra/dynamodb-infra
        env:
          TF_VAR_environment: "dev"
          TF_VAR_dynamodb_table_meetings: ${{ secrets.DEV_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions: ${{ secrets.DEV_DYNAMODB_TABLE_ACTIONS_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if DynamoDB tables already exist
          MEETINGS_TABLE="transinia-dev-${{ secrets.DEV_DYNAMODB_TABLE_MEETINGS_BASE }}"
          ACTIONS_TABLE="transinia-dev-${{ secrets.DEV_DYNAMODB_TABLE_ACTIONS_BASE }}"
          
          MEETINGS_EXISTS=$(aws dynamodb describe-table --table-name "$MEETINGS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          ACTIONS_EXISTS=$(aws dynamodb describe-table --table-name "$ACTIONS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$MEETINGS_EXISTS" = "NOT_FOUND" ] || [ "$ACTIONS_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more DynamoDB tables don't exist. Creating infrastructure..."
            terraform plan -out=dynamodb_tf.plan
            terraform apply -auto-approve dynamodb_tf.plan
          else
            echo "DynamoDB tables already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi

      # Deploy S3 bucket infrastructure
      - name: Deploy S3 Bucket Infrastructure
        working-directory: ./backend/infra/s3-bucket-infra
        env:
          TF_VAR_environment: "dev"
          TF_VAR_s3_bucket_raw: ${{ secrets.DEV_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed: ${{ secrets.DEV_S3_BUCKET_PROCESSED_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if S3 buckets already exist
          RAW_BUCKET="transinia-dev-${{ secrets.DEV_S3_BUCKET_RAW_BASE }}"
          PROCESSED_BUCKET="transinia-dev-${{ secrets.DEV_S3_BUCKET_PROCESSED_BASE }}"
          
          RAW_EXISTS=$(aws s3api head-bucket --bucket "$RAW_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          PROCESSED_EXISTS=$(aws s3api head-bucket --bucket "$PROCESSED_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          
          if [ "$RAW_EXISTS" = "NOT_FOUND" ] || [ "$PROCESSED_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more S3 buckets don't exist. Creating infrastructure..."
            terraform plan -out=s3_tf.plan
            terraform apply -auto-approve s3_tf.plan
          else
            echo "S3 buckets already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi
      
      - name: Build and push backend image
        id: build-backend
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest

      - name: Build and push frontend image
        id: build-frontend
        uses: docker/build-push-action@v4
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest
          build-args: |
            NEXT_PUBLIC_BACKEND_URL=${{ env.BACKEND_API_URL }}
            NEXT_PUBLIC_API_URL=${{ env.BACKEND_API_URL }}

      - name: Create CloudWatch Log Groups
        run: |
          echo "Creating CloudWatch Log Groups..."
          # Create log groups with explicit paths and region
          aws logs create-log-group --log-group-name "/ecs/transinia-dev-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-dev-frontend" --region ${{ env.AWS_REGION }} || true
          echo "Log groups created or already exist"

      # Task definition steps removed - Terraform will handle ECS deployment
          
      # Check and ensure ECS cluster exists
      - name: Ensure ECS cluster exists
        run: |
          echo "Checking if ECS cluster exists..."
          CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text 2>/dev/null || echo "MISSING")
          
          if [ "$CLUSTER_STATUS" = "MISSING" ]; then
            echo "Creating ECS cluster ${{ env.CLUSTER_NAME }}..."
            aws ecs create-cluster --cluster-name ${{ env.CLUSTER_NAME }}
            
            # Verify cluster status
            CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text)
            echo "New cluster status: $CLUSTER_STATUS"
            
            if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
              echo "Error: Cluster is not in ACTIVE state after creation."
              exit 1
            fi
          else
            echo "ECS cluster already exists with status: $CLUSTER_STATUS"
            
            # Check for any services that should be removed
            SERVICES=$(aws ecs list-services --cluster ${{ env.CLUSTER_NAME }} --query 'serviceArns' --output text)
            if [ -n "$SERVICES" ]; then
              echo "Found existing services in the cluster. These will be managed by Terraform."
            fi
          fi
          
          echo "ECS cluster is ready for use."

      # Deploy ECS Fargate infrastructure
      - name: Deploy ECS Fargate Infrastructure
        working-directory: ./backend/infra/ecs-fargate-infra
        env:
          TF_VAR_environment: "dev"
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_backend_ecr_repo: ${{ env.BACKEND_ECR_REPO }}
          TF_VAR_frontend_ecr_repo: ${{ env.FRONTEND_ECR_REPO }}
          TF_VAR_backend_image: "${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest"
          TF_VAR_frontend_image: "${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest"
          TF_VAR_aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          TF_VAR_dynamodb_table_meetings_base: ${{ secrets.DEV_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions_base: ${{ secrets.DEV_DYNAMODB_TABLE_ACTIONS_BASE }}
          TF_VAR_s3_bucket_raw_base: ${{ secrets.DEV_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed_base: ${{ secrets.DEV_S3_BUCKET_PROCESSED_BASE }}
        run: |
          echo "Cleaning up any half-created resources before proceeding..."
          
          # Cleanup potential dangling ECS services
          BACKEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.BACKEND_SERVICE }} --query "services[?status!='INACTIVE'].serviceName" --output text 2>/dev/null || echo "")
          FRONTEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.FRONTEND_SERVICE }} --query "services[?status!='INACTIVE'].serviceName" --output text 2>/dev/null || echo "")
          
          if [ -n "$BACKEND_SERVICE_EXISTS" ]; then
            echo "Cleaning up existing backend service..."
            aws ecs update-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.BACKEND_SERVICE }} --desired-count 0 || true
            aws ecs delete-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.BACKEND_SERVICE }} --force || true
            echo "Waiting for service deletion..."
            sleep 10
          fi
          
          if [ -n "$FRONTEND_SERVICE_EXISTS" ]; then
            echo "Cleaning up existing frontend service..."
            aws ecs update-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.FRONTEND_SERVICE }} --desired-count 0 || true
            aws ecs delete-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.FRONTEND_SERVICE }} --force || true
            echo "Waiting for service deletion..."
            sleep 10
          fi
          
          # Initialize Terraform
          echo "Initializing Terraform..."
          terraform init -upgrade
          
          # Check if all required resources exist
          echo "Checking if all required resources exist..."
          
          # Check VPC
          VPC_EXISTS=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=transinia-dev-vpc" --query "Vpcs[0].VpcId" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check roles
          TASK_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-dev-ecsTaskRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          EXEC_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-dev-ecsTaskExecutionRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check load balancer
          ALB_EXISTS=$(aws elbv2 describe-load-balancers --names "transinia-dev-alb" --query "LoadBalancers[0].LoadBalancerArn" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check CloudWatch log groups
          BACKEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-dev-backend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          FRONTEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-dev-frontend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Ensure CloudWatch log groups exist
          if [ "$BACKEND_LOG_GROUP_EXISTS" = "NOT_FOUND" ]; then
            echo "Creating backend log group..."
            aws logs create-log-group --log-group-name "/ecs/transinia-dev-backend" --region ${{ env.AWS_REGION }}
          fi
          
          if [ "$FRONTEND_LOG_GROUP_EXISTS" = "NOT_FOUND" ]; then
            echo "Creating frontend log group..."
            aws logs create-log-group --log-group-name "/ecs/transinia-dev-frontend" --region ${{ env.AWS_REGION }}
          fi
          
          # Run Terraform in import-only mode if all infrastructure exists
          if [ "$VPC_EXISTS" != "NOT_FOUND" ] && [ "$TASK_ROLE_EXISTS" != "NOT_FOUND" ] && [ "$EXEC_ROLE_EXISTS" != "NOT_FOUND" ] && [ "$ALB_EXISTS" != "NOT_FOUND" ]; then
            echo "All required infrastructure exists. Setting variables for existing infrastructure..."
            
            # Set variables to use existing resources
            export TF_VAR_create_vpc_resources="false"
            export TF_VAR_create_iam_resources="false" 
            export TF_VAR_create_alb_resources="false"
            export TF_VAR_create_cloudwatch_log_groups="false"
            export TF_VAR_infrastructure_creation_strategy="use_existing"
            export TF_VAR_use_existing_vpc_resources="true"
            export TF_VAR_use_existing_task_definitions="false" # Always false since we want to deploy new task definitions
            
            echo "Planning deployment to update services with latest images..."
            terraform plan -refresh-only -out=refresh.tfplan
            
            echo "Running deployment to update services..."
            terraform apply -auto-approve -var="infrastructure_creation_strategy=use_existing" \
              -var="use_existing_vpc_resources=true" \
              -var="use_existing_task_definitions=false" \
              -var="create_vpc_resources=false" \
              -var="create_iam_resources=false" \
              -var="create_alb_resources=false" \
              -var="create_cloudwatch_log_groups=false"
          else
            echo "ERROR: Required infrastructure not found. Please create all required resources manually before running this workflow."
            echo "Missing resources:"
            [ "$VPC_EXISTS" = "NOT_FOUND" ] && echo "- VPC 'transinia-dev-vpc'"
            [ "$TASK_ROLE_EXISTS" = "NOT_FOUND" ] && echo "- IAM Role 'transinia-dev-ecsTaskRole'"
            [ "$EXEC_ROLE_EXISTS" = "NOT_FOUND" ] && echo "- IAM Role 'transinia-dev-ecsTaskExecutionRole'"
            [ "$ALB_EXISTS" = "NOT_FOUND" ] && echo "- ALB 'transinia-dev-alb'"
            exit 1
          fi
          
          echo "Deployment successful. Verifying resources..."
          aws ecs list-services --cluster ${{ env.CLUSTER_NAME }}

      # Comprehensive cleanup step - always runs even if prior steps fail
      - name: Cleanup
        if: always()
        run: |
          echo "Performing comprehensive cleanup operations..."
          
          # Ensure log groups exist
          aws logs create-log-group --log-group-name "/ecs/transinia-dev-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-dev-frontend" --region ${{ env.AWS_REGION }} || true
          
          # Stop and clean up failed tasks
          echo "Cleaning up failed tasks..."
          FAILED_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status STOPPED --query 'taskArns[*]' --output text || echo "")
          if [[ ! -z "$FAILED_TASKS" ]]; then
            echo "Found failed tasks: $FAILED_TASKS"
            for TASK in $FAILED_TASKS; do
              aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK --region ${{ env.AWS_REGION }} || true
            done
          fi
          
          # Check and clean up any running tasks that aren't part of a service
          echo "Checking for orphaned running tasks..."
          RUNNING_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status RUNNING --query 'taskArns[*]' --output text || echo "")
          if [[ ! -z "$RUNNING_TASKS" ]]; then
            for TASK in $RUNNING_TASKS; do
              TASK_SERVICE=$(aws ecs describe-tasks --cluster ${{ env.CLUSTER_NAME }} --tasks $TASK --query 'tasks[0].group' --output text || echo "")
              if [[ ! $TASK_SERVICE == service:* ]]; then
                echo "Stopping orphaned task: $TASK"
                aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK --region ${{ env.AWS_REGION }} || true
              fi
            done
          fi
