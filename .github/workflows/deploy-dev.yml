name: CI/CD â€” build, push to ECR, deploy to ECS (dev)

on:
  push:
    branches: [ dev ]

env:
  ENVIRONMENT: dev
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  ECR_REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com
  BACKEND_ECR_REPO: transinia-dev-backend
  FRONTEND_ECR_REPO: transinia-dev-frontend
  CLUSTER_NAME: transinia-dev-cluster
  BACKEND_SERVICE: transinia-dev-backend-service
  FRONTEND_SERVICE: transinia-dev-frontend-service
  BACKEND_API_URL: http://transinia-dev-alb-1892657611.us-east-1.elb.amazonaws.com

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
    # Add always() to ensure cleanup runs even if job fails
    if: always()

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Create ECR repositories if missing
        run: |
          aws ecr describe-repositories --repository-names $BACKEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $BACKEND_ECR_REPO
          aws ecr describe-repositories --repository-names $FRONTEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $FRONTEND_ECR_REPO

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v1
        id: ecr-login
        with:
          mask-password: 'true'

      # Set up Terraform
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.5

      # Deploy DynamoDB infrastructure
      - name: Deploy DynamoDB Infrastructure
        working-directory: ./backend/infra/dynamodb-infra
        env:
          TF_VAR_environment: "dev"
          TF_VAR_dynamodb_table_meetings: ${{ secrets.DEV_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions: ${{ secrets.DEV_DYNAMODB_TABLE_ACTIONS_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if DynamoDB tables already exist
          MEETINGS_TABLE="transinia-dev-${{ secrets.DEV_DYNAMODB_TABLE_MEETINGS_BASE }}"
          ACTIONS_TABLE="transinia-dev-${{ secrets.DEV_DYNAMODB_TABLE_ACTIONS_BASE }}"
          
          MEETINGS_EXISTS=$(aws dynamodb describe-table --table-name "$MEETINGS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          ACTIONS_EXISTS=$(aws dynamodb describe-table --table-name "$ACTIONS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$MEETINGS_EXISTS" = "NOT_FOUND" ] || [ "$ACTIONS_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more DynamoDB tables don't exist. Creating infrastructure..."
            terraform plan -out=dynamodb_tf.plan
            terraform apply -auto-approve dynamodb_tf.plan
          else
            echo "DynamoDB tables already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi

      # Deploy S3 bucket infrastructure
      - name: Deploy S3 Bucket Infrastructure
        working-directory: ./backend/infra/s3-bucket-infra
        env:
          TF_VAR_environment: "dev"
          TF_VAR_s3_bucket_raw: ${{ secrets.DEV_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed: ${{ secrets.DEV_S3_BUCKET_PROCESSED_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if S3 buckets already exist
          RAW_BUCKET="transinia-dev-${{ secrets.DEV_S3_BUCKET_RAW_BASE }}"
          PROCESSED_BUCKET="transinia-dev-${{ secrets.DEV_S3_BUCKET_PROCESSED_BASE }}"
          
          RAW_EXISTS=$(aws s3api head-bucket --bucket "$RAW_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          PROCESSED_EXISTS=$(aws s3api head-bucket --bucket "$PROCESSED_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          
          if [ "$RAW_EXISTS" = "NOT_FOUND" ] || [ "$PROCESSED_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more S3 buckets don't exist. Creating infrastructure..."
            terraform plan -out=s3_tf.plan
            terraform apply -auto-approve s3_tf.plan
          else
            echo "S3 buckets already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi
      
      - name: Build and push backend image
        id: build-backend
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest

      - name: Build and push frontend image
        id: build-frontend
        uses: docker/build-push-action@v4
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest
          build-args: |
            NEXT_PUBLIC_BACKEND_URL=${{ env.BACKEND_API_URL }}
            NEXT_PUBLIC_API_URL=${{ env.BACKEND_API_URL }}

      - name: Create CloudWatch Log Groups
        run: |
          echo "Creating CloudWatch Log Groups..."
          # Create log groups with explicit paths and region
          aws logs create-log-group --log-group-name "/ecs/transinia-dev-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-dev-frontend" --region ${{ env.AWS_REGION }} || true
          echo "Log groups created or already exist"

      # Task definition steps removed - Terraform will handle ECS deployment
          
      # Check and create ECS resources if they don't exist
      - name: Create ECS cluster and ensure it's active
        run: |
          echo "Checking if ECS cluster exists..."
          # Always delete the cluster first if it exists and recreate it
          # This ensures we have an active cluster
          CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text 2>/dev/null || echo "MISSING")
          
          if [ "$CLUSTER_STATUS" != "MISSING" ]; then
            echo "Deleting existing ECS cluster ${{ env.CLUSTER_NAME }}..."
            # First list and stop any tasks/services
            SERVICES=$(aws ecs list-services --cluster ${{ env.CLUSTER_NAME }} --query 'serviceArns' --output text)
            if [ -n "$SERVICES" ]; then
              for SERVICE in $SERVICES; do
                echo "Deleting service $SERVICE"
                aws ecs update-service --cluster ${{ env.CLUSTER_NAME }} --service $SERVICE --desired-count 0 || true
                aws ecs delete-service --cluster ${{ env.CLUSTER_NAME }} --service $SERVICE --force || true
              done
            fi
            
            # Wait a few seconds to allow services to drain
            echo "Waiting for services to drain..."
            sleep 10
            
            # Delete the cluster
            aws ecs delete-cluster --cluster ${{ env.CLUSTER_NAME }} || true
            
            # Wait for the cluster to be deleted
            echo "Waiting for cluster to be deleted..."
            sleep 20
          fi
          
          echo "Creating fresh ECS cluster ${{ env.CLUSTER_NAME }}..."
          aws ecs create-cluster --cluster-name ${{ env.CLUSTER_NAME }}
          
          # Verify cluster status
          CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text)
          echo "Cluster status: $CLUSTER_STATUS"
          
          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "Error: Cluster is not in ACTIVE state after creation."
            exit 1
          fi
          
          echo "ECS cluster is now ACTIVE and ready for use."

      # Deploy ECS Fargate infrastructure
      - name: Deploy ECS Fargate Infrastructure
        working-directory: ./backend/infra/ecs-fargate-infra
        env:
          TF_VAR_environment: "dev"
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_backend_ecr_repo: ${{ env.BACKEND_ECR_REPO }}
          TF_VAR_frontend_ecr_repo: ${{ env.FRONTEND_ECR_REPO }}
          TF_VAR_backend_image: "${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest"
          TF_VAR_frontend_image: "${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest"
          TF_VAR_aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          TF_VAR_dynamodb_table_meetings_base: ${{ secrets.DEV_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions_base: ${{ secrets.DEV_DYNAMODB_TABLE_ACTIONS_BASE }}
          TF_VAR_s3_bucket_raw_base: ${{ secrets.DEV_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed_base: ${{ secrets.DEV_S3_BUCKET_PROCESSED_BASE }}
        run: |
          # Enable Terraform debug logging
          export TF_LOG=INFO
          
          echo "Initializing Terraform..."
          terraform init -upgrade
          
          # Check if ECS services already exist
          BACKEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.BACKEND_SERVICE }} --query "services[?status=='ACTIVE'].serviceName" --output text 2>/dev/null || echo "NOT_FOUND")
          FRONTEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.FRONTEND_SERVICE }} --query "services[?status=='ACTIVE'].serviceName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ -z "$BACKEND_SERVICE_EXISTS" ] || [ -z "$FRONTEND_SERVICE_EXISTS" ]; then
            echo "One or more ECS services don't exist. Creating infrastructure..."
            echo "Planning Terraform deployment..."
            terraform plan -out=ecs_tf.plan
            
            echo "Applying Terraform changes..."
            terraform apply -auto-approve ecs_tf.plan || {
              echo "Terraform apply failed, showing current state"
              aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0]' || echo "Cluster not found"
              exit 1
            }
          else
            echo "ECS services already exist. Running plan to update services with latest images..."
            # Run plan and apply but don't auto-approve
            terraform plan -out=ecs_tf.plan
            
            # Check if there are any changes
            PLAN_OUTPUT=$(terraform show -no-color ecs_tf.plan)
            if echo "$PLAN_OUTPUT" | grep -q "No changes"; then
              echo "No changes detected. Skipping apply."
            else
              echo "Changes detected. Applying changes to update services..."
              terraform apply -auto-approve ecs_tf.plan
            fi
          fi
          
          echo "Terraform deployment successful. Verifying resources..."
          aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status'
          aws ecs list-services --cluster ${{ env.CLUSTER_NAME }}

      # Cleanup step - always runs even if prior steps fail
      - name: Cleanup
        if: always()
        run: |
          echo "Performing cleanup operations..."
          # Double check the log groups exist
          aws logs create-log-group --log-group-name "/ecs/transinia-dev-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-dev-frontend" --region ${{ env.AWS_REGION }} || true
          
          # Check for any failed tasks and stop them
          FAILED_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status STOPPED --query 'taskArns[*]' --output text || echo "")
          if [[ ! -z "$FAILED_TASKS" ]]; then
            echo "Cleaning up failed tasks: $FAILED_TASKS"
            for TASK in $FAILED_TASKS; do
              aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK --region ${{ env.AWS_REGION }} || true
            done
          fi
