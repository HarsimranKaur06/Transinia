name: CI/CD â€” build, push to ECR, deploy to ECS (production)

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      deploy_reason:
        description: 'Reason for manual deployment'
        required: true
        default: 'Manual production deployment'

env:
  ENVIRONMENT: prod
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  ECR_REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com
  BACKEND_ECR_REPO: transinia-prod-backend
  FRONTEND_ECR_REPO: transinia-prod-frontend
  CLUSTER_NAME: transinia-prod-cluster
  BACKEND_SERVICE: transinia-prod-backend-service
  FRONTEND_SERVICE: transinia-prod-frontend-service
  BACKEND_API_URL: http://transinia-prod-alb.us-east-1.elb.amazonaws.com

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
    if: always()

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Create ECR repositories if missing
        run: |
          aws ecr describe-repositories --repository-names $BACKEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $BACKEND_ECR_REPO
          aws ecr describe-repositories --repository-names $FRONTEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $FRONTEND_ECR_REPO

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v1
        id: ecr-login
        with:
          mask-password: 'true'

      - name: Build and push backend image
        id: build-backend
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest
          build-args: |
            ENVIRONMENT=${{ env.ENVIRONMENT }}

      - name: Build and push frontend image
        id: build-frontend
        uses: docker/build-push-action@v4
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest
          build-args: |
            NEXT_PUBLIC_BACKEND_URL=${{ env.BACKEND_API_URL }}
            NEXT_PUBLIC_API_URL=${{ env.BACKEND_API_URL }}

      - name: Create CloudWatch Log Groups
        run: |
          echo "Creating CloudWatch Log Groups..."
          # Create log groups with explicit paths and region
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-frontend" --region ${{ env.AWS_REGION }} || true
          echo "Log groups created or already exist"

      # Deploy the backend service
      - name: Update backend service
        run: |
          aws ecs update-service --cluster $CLUSTER_NAME --service $BACKEND_SERVICE --force-new-deployment

      # Deploy the frontend service
      - name: Update frontend service
        run: |
          aws ecs update-service --cluster $CLUSTER_NAME --service $FRONTEND_SERVICE --force-new-deployment

      - name: Wait for services to stabilize
        run: |
          aws ecs wait services-stable --cluster $CLUSTER_NAME --services $BACKEND_SERVICE
          aws ecs wait services-stable --cluster $CLUSTER_NAME --services $FRONTEND_SERVICE

      # Deploy Terraform Infrastructure
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.5

      # Deploy DynamoDB infrastructure
      - name: Deploy DynamoDB Infrastructure
        working-directory: ./backend/infra/dynamodb-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_dynamodb_table_meetings: ${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions: ${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if DynamoDB tables already exist
          MEETINGS_TABLE="transinia-prod-${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}"
          ACTIONS_TABLE="transinia-prod-${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}"
          
          MEETINGS_EXISTS=$(aws dynamodb describe-table --table-name "$MEETINGS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          ACTIONS_EXISTS=$(aws dynamodb describe-table --table-name "$ACTIONS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$MEETINGS_EXISTS" = "NOT_FOUND" ] || [ "$ACTIONS_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more DynamoDB tables don't exist. Creating infrastructure..."
            terraform plan -out=dynamodb_tf.plan
            terraform apply -auto-approve dynamodb_tf.plan
          else
            echo "DynamoDB tables already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi

      # Deploy S3 bucket infrastructure
      - name: Deploy S3 Bucket Infrastructure
        working-directory: ./backend/infra/s3-bucket-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_s3_bucket_raw: ${{ secrets.PROD_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed: ${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if S3 buckets already exist
          RAW_BUCKET="transinia-prod-${{ secrets.PROD_S3_BUCKET_RAW_BASE }}"
          PROCESSED_BUCKET="transinia-prod-${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}"
          
          RAW_EXISTS=$(aws s3api head-bucket --bucket "$RAW_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          PROCESSED_EXISTS=$(aws s3api head-bucket --bucket "$PROCESSED_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          
          if [ "$RAW_EXISTS" = "NOT_FOUND" ] || [ "$PROCESSED_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more S3 buckets don't exist. Creating infrastructure..."
            terraform plan -out=s3_tf.plan
            terraform apply -auto-approve s3_tf.plan
          else
            echo "S3 buckets already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi

      # Create ECS cluster and ensure it's active
      - name: Create ECS cluster and ensure it's active
        run: |
          echo "Checking if ECS cluster exists..."
          # Always delete the cluster first if it exists and recreate it
          # This ensures we have an active cluster
          CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text 2>/dev/null || echo "MISSING")
          
          if [ "$CLUSTER_STATUS" != "MISSING" ]; then
            echo "Deleting existing ECS cluster ${{ env.CLUSTER_NAME }}..."
            # First list and stop any tasks/services
            SERVICES=$(aws ecs list-services --cluster ${{ env.CLUSTER_NAME }} --query 'serviceArns' --output text)
            if [ -n "$SERVICES" ]; then
              for SERVICE in $SERVICES; do
                echo "Deleting service $SERVICE"
                aws ecs update-service --cluster ${{ env.CLUSTER_NAME }} --service $SERVICE --desired-count 0 || true
                aws ecs delete-service --cluster ${{ env.CLUSTER_NAME }} --service $SERVICE --force || true
              done
            fi
            
            # Wait a few seconds to allow services to drain
            echo "Waiting for services to drain..."
            sleep 10
            
            # Delete the cluster
            aws ecs delete-cluster --cluster ${{ env.CLUSTER_NAME }} || true
            
            # Wait for the cluster to be deleted
            echo "Waiting for cluster to be deleted..."
            sleep 20
          fi
          
          echo "Creating fresh ECS cluster ${{ env.CLUSTER_NAME }}..."
          aws ecs create-cluster --cluster-name ${{ env.CLUSTER_NAME }}
          
          # Verify cluster status
          CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text)
          echo "Cluster status: $CLUSTER_STATUS"
          
          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "Error: Cluster is not in ACTIVE state after creation."
            exit 1
          fi
          
          echo "ECS cluster is now ACTIVE and ready for use."

      # Deploy ECS Fargate infrastructure
      - name: Deploy ECS Fargate Infrastructure
        working-directory: ./backend/infra/ecs-fargate-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_backend_ecr_repo: ${{ env.BACKEND_ECR_REPO }}
          TF_VAR_frontend_ecr_repo: ${{ env.FRONTEND_ECR_REPO }}
          TF_VAR_backend_image: "${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest"
          TF_VAR_frontend_image: "${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest"
          TF_VAR_aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          TF_VAR_dynamodb_table_meetings_base: ${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions_base: ${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}
          TF_VAR_s3_bucket_raw_base: ${{ secrets.PROD_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed_base: ${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}
        run: |
          # Enable Terraform debug logging
          export TF_LOG=INFO
          
          echo "Initializing Terraform..."
          terraform init -upgrade
          
          # Check if ECS services already exist
          BACKEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.BACKEND_SERVICE }} --query "services[?status=='ACTIVE'].serviceName" --output text 2>/dev/null || echo "NOT_FOUND")
          FRONTEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.FRONTEND_SERVICE }} --query "services[?status=='ACTIVE'].serviceName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check if CloudWatch log groups exist
          BACKEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/${{ env.CLUSTER_NAME }}-backend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          FRONTEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/${{ env.CLUSTER_NAME }}-frontend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check if VPC resources exist
          VPC_EXISTS=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=transinia-prod-vpc" --query "Vpcs[0].VpcId" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check if IAM roles exist
          TASK_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-prod-ecsTaskRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          EXEC_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-prod-ecsTaskExecutionRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Set variables based on existing resources
          if [ "$BACKEND_LOG_GROUP_EXISTS" != "NOT_FOUND" ] || [ "$FRONTEND_LOG_GROUP_EXISTS" != "NOT_FOUND" ]; then
            echo "CloudWatch log groups already exist. Setting create_cloudwatch_log_groups=false"
            CREATE_CLOUDWATCH="false"
          else
            echo "CloudWatch log groups don't exist. Will create them."
            CREATE_CLOUDWATCH="true"
          fi
          
          if [ "$VPC_EXISTS" != "NOT_FOUND" ]; then
            echo "VPC already exists. Setting create_vpc_resources=false"
            CREATE_VPC="false"
          else
            echo "VPC doesn't exist. Will create it."
            CREATE_VPC="true"
          fi
          
          if [ "$TASK_ROLE_EXISTS" != "NOT_FOUND" ] && [ "$EXEC_ROLE_EXISTS" != "NOT_FOUND" ]; then
            echo "IAM roles already exist. Setting create_iam_resources=false"
            CREATE_IAM="false"
          else
            echo "IAM roles don't exist. Will create them."
            CREATE_IAM="true"
          fi
          
          # Set our new infrastructure creation strategy variable
          if [ "$CREATE_VPC" = "true" ] && [ "$CREATE_IAM" = "true" ] && [ "$CREATE_CLOUDWATCH" = "true" ]; then
            # If everything needs to be created
            INFRASTRUCTURE_STRATEGY="create_all"
          elif [ "$CREATE_VPC" = "false" ] && [ "$CREATE_IAM" = "false" ] && [ "$CREATE_CLOUDWATCH" = "false" ]; then
            # If everything already exists
            INFRASTRUCTURE_STRATEGY="use_existing"
          else
            # Mixed scenario
            INFRASTRUCTURE_STRATEGY="mixed"
          fi
          
          # Set use_existing flags based on infrastructure strategy
          USE_EXISTING_VPC="false"
          USE_EXISTING_TASK_DEFS="false"
          if [ "$INFRASTRUCTURE_STRATEGY" = "use_existing" ] || [ "$INFRASTRUCTURE_STRATEGY" = "mixed" ]; then
            # If we're using existing resources, enable the data sources
            USE_EXISTING_VPC="true"
            USE_EXISTING_TASK_DEFS="true"
          fi
          
          # Combine all variables
          TERRAFORM_VARS="-var=infrastructure_creation_strategy=$INFRASTRUCTURE_STRATEGY -var=use_existing_vpc_resources=$USE_EXISTING_VPC -var=use_existing_task_definitions=$USE_EXISTING_TASK_DEFS -var=create_cloudwatch_log_groups=$CREATE_CLOUDWATCH -var=create_vpc_resources=$CREATE_VPC -var=create_iam_resources=$CREATE_IAM -var=create_alb_resources=$CREATE_VPC"
          
          # Print the variables we're using
          echo "Using the following Terraform variables:"
          echo "infrastructure_creation_strategy = $INFRASTRUCTURE_STRATEGY"
          echo "use_existing_vpc_resources = $USE_EXISTING_VPC"
          echo "use_existing_task_definitions = $USE_EXISTING_TASK_DEFS"
          echo "create_cloudwatch_log_groups = $CREATE_CLOUDWATCH"
          echo "create_vpc_resources = $CREATE_VPC"
          echo "create_iam_resources = $CREATE_IAM"
          echo "create_alb_resources = $CREATE_VPC"
          
          if [ -z "$BACKEND_SERVICE_EXISTS" ] || [ -z "$FRONTEND_SERVICE_EXISTS" ]; then
            echo "One or more ECS services don't exist. Creating infrastructure..."
            echo "Planning Terraform deployment..."
            terraform plan $TERRAFORM_VARS -out=ecs_tf.plan
            
            echo "Applying Terraform changes..."
            terraform apply -auto-approve ecs_tf.plan || {
              echo "Terraform apply failed, showing current state"
              aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0]' || echo "Cluster not found"
              exit 1
            }
          else
            echo "ECS services already exist. Running plan to update services with latest images..."
            # Run plan and apply but don't auto-approve
            terraform plan $TERRAFORM_VARS -out=ecs_tf.plan
            
            # Check if there are any changes
            PLAN_OUTPUT=$(terraform show -no-color ecs_tf.plan)
            if echo "$PLAN_OUTPUT" | grep -q "No changes"; then
              echo "No changes detected. Skipping apply."
            else
              echo "Changes detected. Applying changes to update services..."
              terraform apply -auto-approve ecs_tf.plan
            fi
          fi
          
          echo "Terraform deployment successful. Verifying resources..."
          aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status'
          aws ecs list-services --cluster ${{ env.CLUSTER_NAME }}

      # Cleanup step - always runs even if prior steps fail
      - name: Cleanup
        if: always()
        run: |
          echo "Performing cleanup operations..."
          # Double check the log groups exist
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-frontend" --region ${{ env.AWS_REGION }} || true
          
          # Check for any failed tasks and stop them
          FAILED_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status STOPPED --query 'taskArns[*]' --output text || echo "")
          if [[ ! -z "$FAILED_TASKS" ]]; then
            echo "Cleaning up failed tasks: $FAILED_TASKS"
            for TASK in $FAILED_TASKS; do
              aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK --region ${{ env.AWS_REGION }} || true
            done
          fi

      - name: Deployment Success
        run: echo "Production deployment completed successfully!"