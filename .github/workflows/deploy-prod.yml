name: Destroy All Infrastructure (production)

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      confirm_destroy:
        description: 'Type YES to confirm destroying all infrastructure'
        required: true
        default: 'NO'

env:
  ENVIRONMENT: prod
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  ECR_REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com
  BACKEND_ECR_REPO: transinia-prod-backend
  FRONTEND_ECR_REPO: transinia-prod-frontend
  CLUSTER_NAME: transinia-prod-cluster
  BACKEND_SERVICE: transinia-prod-backend-service
  FRONTEND_SERVICE: transinia-prod-frontend-service
  BACKEND_API_URL: http://transinia-prod-alb.us-east-1.elb.amazonaws.com

jobs:
  destroy-all-infrastructure:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.confirm_destroy == 'YES' || github.event_name == 'push'

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Create ECR repositories if missing
        run: |
          aws ecr describe-repositories --repository-names $BACKEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $BACKEND_ECR_REPO
          aws ecr describe-repositories --repository-names $FRONTEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $FRONTEND_ECR_REPO

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v1
        id: ecr-login
        with:
          mask-password: 'true'

      - name: Build and push backend image
        id: build-backend
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest
          build-args: |
            ENVIRONMENT=${{ env.ENVIRONMENT }}

      - name: Build and push frontend image
        id: build-frontend
        uses: docker/build-push-action@v4
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest
          build-args: |
            NEXT_PUBLIC_BACKEND_URL=${{ env.BACKEND_API_URL }}
            NEXT_PUBLIC_API_URL=${{ env.BACKEND_API_URL }}

      - name: Create CloudWatch Log Groups
        run: |
          echo "Creating CloudWatch Log Groups..."
          # Create log groups with explicit paths and region
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-frontend" --region ${{ env.AWS_REGION }} || true
          echo "Log groups created or already exist"

      # Deploy the backend service
      - name: Update backend service
        run: |
          aws ecs update-service --cluster $CLUSTER_NAME --service $BACKEND_SERVICE --force-new-deployment

      # Deploy the frontend service
      - name: Update frontend service
        run: |
          aws ecs update-service --cluster $CLUSTER_NAME --service $FRONTEND_SERVICE --force-new-deployment

      - name: Wait for services to stabilize
        run: |
          aws ecs wait services-stable --cluster $CLUSTER_NAME --services $BACKEND_SERVICE
          aws ecs wait services-stable --cluster $CLUSTER_NAME --services $FRONTEND_SERVICE

      # Deploy Terraform Infrastructure
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.5

      # Deploy DynamoDB infrastructure
      - name: Deploy DynamoDB Infrastructure
        working-directory: ./backend/infra/dynamodb-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_dynamodb_table_meetings: ${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions: ${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if DynamoDB tables already exist
          MEETINGS_TABLE="transinia-prod-${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}"
          ACTIONS_TABLE="transinia-prod-${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}"
          
          MEETINGS_EXISTS=$(aws dynamodb describe-table --table-name "$MEETINGS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          ACTIONS_EXISTS=$(aws dynamodb describe-table --table-name "$ACTIONS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$MEETINGS_EXISTS" = "NOT_FOUND" ] || [ "$ACTIONS_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more DynamoDB tables don't exist. Creating infrastructure..."
            terraform plan -out=dynamodb_tf.plan
            terraform apply -auto-approve dynamodb_tf.plan
          else
            echo "DynamoDB tables already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi

      # Deploy S3 bucket infrastructure
      - name: Deploy S3 Bucket Infrastructure
        working-directory: ./backend/infra/s3-bucket-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_s3_bucket_raw: ${{ secrets.PROD_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed: ${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if S3 buckets already exist
          RAW_BUCKET="transinia-prod-${{ secrets.PROD_S3_BUCKET_RAW_BASE }}"
          PROCESSED_BUCKET="transinia-prod-${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}"
          
          RAW_EXISTS=$(aws s3api head-bucket --bucket "$RAW_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          PROCESSED_EXISTS=$(aws s3api head-bucket --bucket "$PROCESSED_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          
          if [ "$RAW_EXISTS" = "NOT_FOUND" ] || [ "$PROCESSED_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more S3 buckets don't exist. Creating infrastructure..."
            terraform plan -out=s3_tf.plan
            terraform apply -auto-approve s3_tf.plan
          else
            echo "S3 buckets already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi

      # Check and ensure ECS cluster exists
      - name: Ensure ECS cluster exists
        run: |
          echo "Checking if ECS cluster exists..."
          CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text 2>/dev/null || echo "MISSING")
          
          if [ "$CLUSTER_STATUS" = "MISSING" ]; then
            echo "Creating ECS cluster ${{ env.CLUSTER_NAME }}..."
            aws ecs create-cluster --cluster-name ${{ env.CLUSTER_NAME }}
            
            # Verify cluster status
            CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text)
            echo "New cluster status: $CLUSTER_STATUS"
            
            if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
              echo "Error: Cluster is not in ACTIVE state after creation."
              exit 1
            fi
          else
            echo "ECS cluster already exists with status: $CLUSTER_STATUS"
            
            # Check for any services that should be removed
            SERVICES=$(aws ecs list-services --cluster ${{ env.CLUSTER_NAME }} --query 'serviceArns' --output text)
            if [ -n "$SERVICES" ]; then
              echo "Found existing services in the cluster. These will be managed by Terraform."
            fi
          fi
          
          echo "ECS cluster is ready for use."

      # Deploy ECS Fargate infrastructure
      - name: Prepare for Manual Infrastructure Setup
        working-directory: ./backend/infra/ecs-fargate-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_backend_ecr_repo: ${{ env.BACKEND_ECR_REPO }}
          TF_VAR_frontend_ecr_repo: ${{ env.FRONTEND_ECR_REPO }}
          TF_VAR_backend_image: "${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest"
          TF_VAR_frontend_image: "${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest"
          TF_VAR_aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          TF_VAR_dynamodb_table_meetings_base: ${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions_base: ${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}
          TF_VAR_s3_bucket_raw_base: ${{ secrets.PROD_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed_base: ${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}
        run: |
          echo "Preparing environment for manual infrastructure setup..."
          echo "Step 1: Cleaning up any existing resources that might conflict..."
          
          # First check if cluster exists before attempting to clean services
          CLUSTER_EXISTS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query "clusters[0].status" --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$CLUSTER_EXISTS" != "NOT_FOUND" ]; then
            # Cleanup potential dangling ECS services
            echo "Checking for existing ECS services..."
            BACKEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.BACKEND_SERVICE }} --query "services[?status!='INACTIVE'].serviceName" --output text 2>/dev/null || echo "")
            FRONTEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.FRONTEND_SERVICE }} --query "services[?status!='INACTIVE'].serviceName" --output text 2>/dev/null || echo "")
            
            if [ -n "$BACKEND_SERVICE_EXISTS" ]; then
              echo "Cleaning up existing backend service..."
              aws ecs update-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.BACKEND_SERVICE }} --desired-count 0 || true
              aws ecs delete-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.BACKEND_SERVICE }} --force || true
              echo "Waiting for service deletion..."
              sleep 10
            fi
            
            if [ -n "$FRONTEND_SERVICE_EXISTS" ]; then
              echo "Cleaning up existing frontend service..."
              aws ecs update-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.FRONTEND_SERVICE }} --desired-count 0 || true
              aws ecs delete-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.FRONTEND_SERVICE }} --force || true
              echo "Waiting for service deletion..."
              sleep 10
            fi
            
            # Check and clean up any orphaned tasks
            echo "Cleaning up any orphaned tasks..."
            RUNNING_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status RUNNING --query 'taskArns[*]' --output text 2>/dev/null || echo "")
            if [ -n "$RUNNING_TASKS" ]; then
              for TASK in $RUNNING_TASKS; do
                echo "Stopping task: $TASK"
                aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK || true
              done
              echo "Waiting for tasks to stop..."
              sleep 10
            fi
          else
            echo "ECS cluster doesn't exist yet, skipping ECS service cleanup."
          fi
          
          # Check for existing resources and generate a status report
          echo "Step 2: Checking for existing infrastructure resources..."
          
          # Check VPC
          VPC_EXISTS=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=transinia-prod-vpc" --query "Vpcs[0].VpcId" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check roles
          TASK_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-prod-ecsTaskRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          EXEC_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-prod-ecsTaskExecutionRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check load balancer
          ALB_EXISTS=$(aws elbv2 describe-load-balancers --names "transinia-prod-alb" --query "LoadBalancers[0].LoadBalancerArn" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check CloudWatch log groups
          BACKEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-prod-backend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          FRONTEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-prod-frontend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Ensure CloudWatch log groups exist (these are simple to create)
          if [ "$BACKEND_LOG_GROUP_EXISTS" = "NOT_FOUND" ]; then
            echo "Creating backend log group..."
            aws logs create-log-group --log-group-name "/ecs/transinia-prod-backend" --region ${{ env.AWS_REGION }}
          fi
          
          if [ "$FRONTEND_LOG_GROUP_EXISTS" = "NOT_FOUND" ]; then
            echo "Creating frontend log group..."
            aws logs create-log-group --log-group-name "/ecs/transinia-prod-frontend" --region ${{ env.AWS_REGION }}
          fi
          
          # Print infrastructure status report
          echo "============================================"
          echo "INFRASTRUCTURE STATUS REPORT"
          echo "============================================"
          echo "ECS Cluster (transinia-prod-cluster): $([ "$CLUSTER_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "VPC (transinia-prod-vpc): $([ "$VPC_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "IAM Task Role (transinia-prod-ecsTaskRole): $([ "$TASK_ROLE_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "IAM Execution Role (transinia-prod-ecsTaskExecutionRole): $([ "$EXEC_ROLE_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "Load Balancer (transinia-prod-alb): $([ "$ALB_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "Backend Log Group: $([ "$BACKEND_LOG_GROUP_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "CREATED")"
          echo "Frontend Log Group: $([ "$FRONTEND_LOG_GROUP_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "CREATED")"
          echo "============================================"
          
          echo "Step 3: Initialize Terraform for future use..."
          # Initialize Terraform (this won't create any resources)
          terraform init -upgrade
          
          echo "Environment is now ready for manual infrastructure setup."
          echo "Please create the required infrastructure resources manually using the AWS CLI or console."
          echo ""
          echo "Once resources are created, you can run the workflow again to deploy the application."
          
          # Successful exit even if infrastructure doesn't exist yet
          exit 0

      # Final cleanup step - always runs even if prior steps fail
      - name: Final Resource Cleanup
        if: always()
        run: |
          echo "Performing final cleanup operations..."
          
          # Ensure log groups exist
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-frontend" --region ${{ env.AWS_REGION }} || true
          
          # Stop and clean up failed tasks
          echo "Cleaning up failed tasks..."
          FAILED_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status STOPPED --query 'taskArns[*]' --output text || echo "")
          if [[ ! -z "$FAILED_TASKS" ]]; then
            echo "Found failed tasks: $FAILED_TASKS"
            for TASK in $FAILED_TASKS; do
              aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK --region ${{ env.AWS_REGION }} || true
            done
          fi
          
          # Check and clean up any running tasks that aren't part of a service
          echo "Checking for orphaned running tasks..."
          RUNNING_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status RUNNING --query 'taskArns[*]' --output text || echo "")
          if [[ ! -z "$RUNNING_TASKS" ]]; then
            for TASK in $RUNNING_TASKS; do
              TASK_SERVICE=$(aws ecs describe-tasks --cluster ${{ env.CLUSTER_NAME }} --tasks $TASK --query 'tasks[0].group' --output text || echo "")
              if [[ ! $TASK_SERVICE == service:* ]]; then
                echo "Stopping orphaned task: $TASK"
                aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK --region ${{ env.AWS_REGION }} || true
              fi
            done
          fi

      - name: Deployment Success
        run: echo "Production deployment completed successfully!"