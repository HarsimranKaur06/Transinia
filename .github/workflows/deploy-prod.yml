name: Destroy All Infrastructure (production)

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      confirm_destroy:
        description: 'Type YES to confirm destroying all infrastructure'
        required: true
        default: 'NO'

env:
  ENVIRONMENT: prod
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  ECR_REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com
  BACKEND_ECR_REPO: transinia-prod-backend
  FRONTEND_ECR_REPO: transinia-prod-frontend
  CLUSTER_NAME: transinia-prod-cluster
  BACKEND_SERVICE: transinia-prod-backend-service
  FRONTEND_SERVICE: transinia-prod-frontend-service
  BACKEND_API_URL: http://transinia-prod-alb.us-east-1.elb.amazonaws.com

jobs:
  destroy-all-infrastructure:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.confirm_destroy == 'YES' || github.event_name == 'push'

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Create ECR repositories if missing
        run: |
          aws ecr describe-repositories --repository-names $BACKEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $BACKEND_ECR_REPO
          aws ecr describe-repositories --repository-names $FRONTEND_ECR_REPO >/dev/null 2>&1 || aws ecr create-repository --repository-name $FRONTEND_ECR_REPO

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v1
        id: ecr-login
        with:
          mask-password: 'true'

      - name: Build and push backend image
        id: build-backend
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest
          build-args: |
            ENVIRONMENT=${{ env.ENVIRONMENT }}

      - name: Build and push frontend image
        id: build-frontend
        uses: docker/build-push-action@v4
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest
          build-args: |
            NEXT_PUBLIC_BACKEND_URL=${{ env.BACKEND_API_URL }}
            NEXT_PUBLIC_API_URL=${{ env.BACKEND_API_URL }}

      - name: Create CloudWatch Log Groups
        run: |
          echo "Creating CloudWatch Log Groups..."
          # Create log groups with explicit paths and region
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-backend" --region ${{ env.AWS_REGION }} || true
          aws logs create-log-group --log-group-name "/ecs/transinia-prod-frontend" --region ${{ env.AWS_REGION }} || true
          echo "Log groups created or already exist"

      # Deploy the backend service
      - name: Update backend service
        run: |
          aws ecs update-service --cluster $CLUSTER_NAME --service $BACKEND_SERVICE --force-new-deployment

      # Deploy the frontend service
      - name: Update frontend service
        run: |
          aws ecs update-service --cluster $CLUSTER_NAME --service $FRONTEND_SERVICE --force-new-deployment

      - name: Wait for services to stabilize
        run: |
          aws ecs wait services-stable --cluster $CLUSTER_NAME --services $BACKEND_SERVICE
          aws ecs wait services-stable --cluster $CLUSTER_NAME --services $FRONTEND_SERVICE

      # Deploy Terraform Infrastructure
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.5

      # Deploy DynamoDB infrastructure
      - name: Deploy DynamoDB Infrastructure
        working-directory: ./backend/infra/dynamodb-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_dynamodb_table_meetings: ${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions: ${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if DynamoDB tables already exist
          MEETINGS_TABLE="transinia-prod-${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}"
          ACTIONS_TABLE="transinia-prod-${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}"
          
          MEETINGS_EXISTS=$(aws dynamodb describe-table --table-name "$MEETINGS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          ACTIONS_EXISTS=$(aws dynamodb describe-table --table-name "$ACTIONS_TABLE" --query "Table.TableName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$MEETINGS_EXISTS" = "NOT_FOUND" ] || [ "$ACTIONS_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more DynamoDB tables don't exist. Creating infrastructure..."
            terraform plan -out=dynamodb_tf.plan
            terraform apply -auto-approve dynamodb_tf.plan
          else
            echo "DynamoDB tables already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi

      # Deploy S3 bucket infrastructure
      - name: Deploy S3 Bucket Infrastructure
        working-directory: ./backend/infra/s3-bucket-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_s3_bucket_raw: ${{ secrets.PROD_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed: ${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}
        run: |
          terraform init -upgrade
          
          # Check if S3 buckets already exist
          RAW_BUCKET="transinia-prod-${{ secrets.PROD_S3_BUCKET_RAW_BASE }}"
          PROCESSED_BUCKET="transinia-prod-${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}"
          
          RAW_EXISTS=$(aws s3api head-bucket --bucket "$RAW_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          PROCESSED_EXISTS=$(aws s3api head-bucket --bucket "$PROCESSED_BUCKET" 2>/dev/null && echo "EXISTS" || echo "NOT_FOUND")
          
          if [ "$RAW_EXISTS" = "NOT_FOUND" ] || [ "$PROCESSED_EXISTS" = "NOT_FOUND" ]; then
            echo "One or more S3 buckets don't exist. Creating infrastructure..."
            terraform plan -out=s3_tf.plan
            terraform apply -auto-approve s3_tf.plan
          else
            echo "S3 buckets already exist. Skipping creation."
            # Still run plan to verify everything is in sync, but don't apply
            terraform plan
          fi

      # Check and ensure ECS cluster exists
      - name: Ensure ECS cluster exists
        run: |
          echo "Checking if ECS cluster exists..."
          CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text 2>/dev/null || echo "MISSING")
          
          if [ "$CLUSTER_STATUS" = "MISSING" ]; then
            echo "Creating ECS cluster ${{ env.CLUSTER_NAME }}..."
            aws ecs create-cluster --cluster-name ${{ env.CLUSTER_NAME }}
            
            # Verify cluster status
            CLUSTER_STATUS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query 'clusters[0].status' --output text)
            echo "New cluster status: $CLUSTER_STATUS"
            
            if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
              echo "Error: Cluster is not in ACTIVE state after creation."
              exit 1
            fi
          else
            echo "ECS cluster already exists with status: $CLUSTER_STATUS"
            
            # Check for any services that should be removed
            SERVICES=$(aws ecs list-services --cluster ${{ env.CLUSTER_NAME }} --query 'serviceArns' --output text)
            if [ -n "$SERVICES" ]; then
              echo "Found existing services in the cluster. These will be managed by Terraform."
            fi
          fi
          
          echo "ECS cluster is ready for use."

      # Deploy ECS Fargate infrastructure
      - name: Prepare for Manual Infrastructure Setup
        working-directory: ./backend/infra/ecs-fargate-infra
        env:
          TF_VAR_environment: "prod"
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_backend_ecr_repo: ${{ env.BACKEND_ECR_REPO }}
          TF_VAR_frontend_ecr_repo: ${{ env.FRONTEND_ECR_REPO }}
          TF_VAR_backend_image: "${{ env.ECR_REGISTRY }}/${{ env.BACKEND_ECR_REPO }}:latest"
          TF_VAR_frontend_image: "${{ env.ECR_REGISTRY }}/${{ env.FRONTEND_ECR_REPO }}:latest"
          TF_VAR_aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          TF_VAR_dynamodb_table_meetings_base: ${{ secrets.PROD_DYNAMODB_TABLE_MEETINGS_BASE }}
          TF_VAR_dynamodb_table_actions_base: ${{ secrets.PROD_DYNAMODB_TABLE_ACTIONS_BASE }}
          TF_VAR_s3_bucket_raw_base: ${{ secrets.PROD_S3_BUCKET_RAW_BASE }}
          TF_VAR_s3_bucket_processed_base: ${{ secrets.PROD_S3_BUCKET_PROCESSED_BASE }}
        run: |
          echo "Preparing environment for manual infrastructure setup..."
          echo "Step 1: Cleaning up any existing resources that might conflict..."
          
          # First check if cluster exists before attempting to clean services
          CLUSTER_EXISTS=$(aws ecs describe-clusters --clusters ${{ env.CLUSTER_NAME }} --query "clusters[0].status" --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$CLUSTER_EXISTS" != "NOT_FOUND" ]; then
            # Cleanup potential dangling ECS services
            echo "Checking for existing ECS services..."
            BACKEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.BACKEND_SERVICE }} --query "services[?status!='INACTIVE'].serviceName" --output text 2>/dev/null || echo "")
            FRONTEND_SERVICE_EXISTS=$(aws ecs describe-services --cluster ${{ env.CLUSTER_NAME }} --services ${{ env.FRONTEND_SERVICE }} --query "services[?status!='INACTIVE'].serviceName" --output text 2>/dev/null || echo "")
            
            if [ -n "$BACKEND_SERVICE_EXISTS" ]; then
              echo "Cleaning up existing backend service..."
              aws ecs update-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.BACKEND_SERVICE }} --desired-count 0 || true
              aws ecs delete-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.BACKEND_SERVICE }} --force || true
              echo "Waiting for service deletion..."
              sleep 10
            fi
            
            if [ -n "$FRONTEND_SERVICE_EXISTS" ]; then
              echo "Cleaning up existing frontend service..."
              aws ecs update-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.FRONTEND_SERVICE }} --desired-count 0 || true
              aws ecs delete-service --cluster ${{ env.CLUSTER_NAME }} --service ${{ env.FRONTEND_SERVICE }} --force || true
              echo "Waiting for service deletion..."
              sleep 10
            fi
            
            # Check and clean up any orphaned tasks
            echo "Cleaning up any orphaned tasks..."
            RUNNING_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status RUNNING --query 'taskArns[*]' --output text 2>/dev/null || echo "")
            if [ -n "$RUNNING_TASKS" ]; then
              for TASK in $RUNNING_TASKS; do
                echo "Stopping task: $TASK"
                aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK || true
              done
              echo "Waiting for tasks to stop..."
              sleep 10
            fi
          else
            echo "ECS cluster doesn't exist yet, skipping ECS service cleanup."
          fi
          
          # Check for existing resources and generate a status report
          echo "Step 2: Checking for existing infrastructure resources..."
          
          # Check VPC
          VPC_EXISTS=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=transinia-prod-vpc" --query "Vpcs[0].VpcId" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check roles
          TASK_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-prod-ecsTaskRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          EXEC_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-prod-ecsTaskExecutionRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check load balancer
          ALB_EXISTS=$(aws elbv2 describe-load-balancers --names "transinia-prod-alb" --query "LoadBalancers[0].LoadBalancerArn" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Check CloudWatch log groups
          BACKEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-prod-backend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          FRONTEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-prod-frontend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          
          # Ensure CloudWatch log groups exist (these are simple to create)
          if [ "$BACKEND_LOG_GROUP_EXISTS" = "NOT_FOUND" ]; then
            echo "Creating backend log group..."
            aws logs create-log-group --log-group-name "/ecs/transinia-prod-backend" --region ${{ env.AWS_REGION }}
          fi
          
          if [ "$FRONTEND_LOG_GROUP_EXISTS" = "NOT_FOUND" ]; then
            echo "Creating frontend log group..."
            aws logs create-log-group --log-group-name "/ecs/transinia-prod-frontend" --region ${{ env.AWS_REGION }}
          fi
          
          # Print infrastructure status report
          echo "============================================"
          echo "INFRASTRUCTURE STATUS REPORT"
          echo "============================================"
          echo "ECS Cluster (transinia-prod-cluster): $([ "$CLUSTER_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "VPC (transinia-prod-vpc): $([ "$VPC_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "IAM Task Role (transinia-prod-ecsTaskRole): $([ "$TASK_ROLE_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "IAM Execution Role (transinia-prod-ecsTaskExecutionRole): $([ "$EXEC_ROLE_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "Load Balancer (transinia-prod-alb): $([ "$ALB_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "MISSING")"
          echo "Backend Log Group: $([ "$BACKEND_LOG_GROUP_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "CREATED")"
          echo "Frontend Log Group: $([ "$FRONTEND_LOG_GROUP_EXISTS" != "NOT_FOUND" ] && echo "EXISTS" || echo "CREATED")"
          echo "============================================"
          
          echo "Step 3: Initialize Terraform for future use..."
          # Initialize Terraform (this won't create any resources)
          terraform init -upgrade
          
          echo "Environment is now ready for manual infrastructure setup."
          echo "Please create the required infrastructure resources manually using the AWS CLI or console."
          echo ""
          echo "Once resources are created, you can run the workflow again to deploy the application."
          
          # Successful exit even if infrastructure doesn't exist yet
          exit 0

      # Infrastructure Cleanup - destroys all resources
      - name: Destroy Infrastructure
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.confirm_destroy == 'YES'
        run: |
          echo "Starting infrastructure destruction process..."
          
          # Step 1: Delete CloudWatch log groups with improved error handling
          echo "Deleting CloudWatch log groups..."
          
          # Delete backend log group if it exists
          BACKEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-prod-backend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          if [ "$BACKEND_LOG_GROUP_EXISTS" != "NOT_FOUND" ]; then
            echo "Deleting backend log group..."
            aws logs delete-log-group --log-group-name "$BACKEND_LOG_GROUP_EXISTS" --region ${{ env.AWS_REGION }} && echo "Successfully deleted backend log group" || echo "Failed to delete backend log group, it may not exist or you may not have permissions"
          else
            echo "Backend log group not found, skipping deletion"
          fi
          
          # Delete frontend log group if it exists
          FRONTEND_LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-prod-frontend" --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "NOT_FOUND")
          if [ "$FRONTEND_LOG_GROUP_EXISTS" != "NOT_FOUND" ]; then
            echo "Deleting frontend log group..."
            aws logs delete-log-group --log-group-name "$FRONTEND_LOG_GROUP_EXISTS" --region ${{ env.AWS_REGION }} && echo "Successfully deleted frontend log group" || echo "Failed to delete frontend log group, it may not exist or you may not have permissions"
          else
            echo "Frontend log group not found, skipping deletion"
          fi
          
          # Step 2: IAM role cleanup
          echo "Cleaning up IAM roles..."
          
          # Execution Role cleanup
          EXEC_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-prod-ecsTaskExecutionRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          if [ "$EXEC_ROLE_EXISTS" != "NOT_FOUND" ]; then
            echo "Cleaning up execution role 'transinia-prod-ecsTaskExecutionRole'..."
            
            # Step 1: List and detach all policies
            echo "Finding and removing attached policies from execution role..."
            ATTACHED_POLICIES=$(aws iam list-attached-role-policies --role-name "transinia-prod-ecsTaskExecutionRole" --query "AttachedPolicies[*].PolicyArn" --output text 2>/dev/null || echo "")
            if [ -n "$ATTACHED_POLICIES" ]; then
              for POLICY_ARN in $ATTACHED_POLICIES; do
                echo "Detaching policy: $POLICY_ARN"
                aws iam detach-role-policy --role-name "transinia-prod-ecsTaskExecutionRole" --policy-arn "$POLICY_ARN" || true
                sleep 2
              done
            else
              echo "No attached policies found on execution role"
            fi
            
            # Step 2: List and delete inline policies with better error handling
            echo "Finding and removing inline policies..."
            INLINE_POLICIES=$(aws iam list-role-policies --role-name "transinia-prod-ecsTaskExecutionRole" --query "PolicyNames" --output text 2>/dev/null || echo "")
            if [ -n "$INLINE_POLICIES" ]; then
              for POLICY in $INLINE_POLICIES; do
                echo "Deleting inline policy: $POLICY"
                aws iam delete-role-policy --role-name "transinia-prod-ecsTaskExecutionRole" --policy-name "$POLICY" || true
                sleep 2
              done
            else
              echo "No inline policies found"
            fi
            
            # Step 3: Delete the role with improved error handling and multiple retries
            echo "Deleting execution role..."
            
            # Wait a moment for policy detachments to complete
            sleep 5
            
            MAX_RETRIES=5
            for ((i=1; i<=MAX_RETRIES; i++)); do
              # Check if the role still exists
              if aws iam get-role --role-name "transinia-prod-ecsTaskExecutionRole" &>/dev/null; then
                echo "Attempt $i/$MAX_RETRIES: Deleting role 'transinia-prod-ecsTaskExecutionRole'..."
                aws iam delete-role --role-name "transinia-prod-ecsTaskExecutionRole" && { echo "Role successfully deleted"; break; } || echo "Delete attempt $i/$MAX_RETRIES failed, retrying in 15 seconds..."
                sleep 15
              else
                echo "Role 'transinia-prod-ecsTaskExecutionRole' no longer exists, moving on"
                break
              fi
            done
          else
            echo "Execution role 'transinia-prod-ecsTaskExecutionRole' not found, skipping cleanup"
          fi
          
          # Task Role cleanup
          TASK_ROLE_EXISTS=$(aws iam get-role --role-name "transinia-prod-ecsTaskRole" --query "Role.RoleName" --output text 2>/dev/null || echo "NOT_FOUND")
          if [ "$TASK_ROLE_EXISTS" != "NOT_FOUND" ]; then
            echo "Cleaning up task role 'transinia-prod-ecsTaskRole'..."
            
            # Step 1: Check and remove role from instance profiles
            INSTANCE_PROFILES=$(aws iam list-instance-profiles-for-role --role-name "transinia-prod-ecsTaskRole" --query "InstanceProfiles[*].InstanceProfileName" --output text 2>/dev/null || echo "")
            if [ -n "$INSTANCE_PROFILES" ]; then
              for PROFILE in $INSTANCE_PROFILES; do
                echo "Removing role from instance profile: $PROFILE"
                aws iam remove-role-from-instance-profile --instance-profile-name $PROFILE --role-name "transinia-prod-ecsTaskRole" || true
                sleep 2
              done
            fi
            
            # Step 2: List and detach managed policies with better error handling
            echo "Finding and removing attached policies from task role..."
            ATTACHED_POLICIES=$(aws iam list-attached-role-policies --role-name "transinia-prod-ecsTaskRole" --query "AttachedPolicies[*].PolicyArn" --output text 2>/dev/null || echo "")
            if [ -n "$ATTACHED_POLICIES" ]; then
              for POLICY in $ATTACHED_POLICIES; do
                echo "Detaching policy: $POLICY"
                aws iam detach-role-policy --role-name "transinia-prod-ecsTaskRole" --policy-arn "$POLICY" || true
                sleep 2
              done
            else
              echo "No attached policies found on task role"
            fi
            
            # Step 3: List and delete inline policies with better error handling
            echo "Finding and removing inline policies from task role..."
            INLINE_POLICIES=$(aws iam list-role-policies --role-name "transinia-prod-ecsTaskRole" --query "PolicyNames" --output text 2>/dev/null || echo "")
            if [ -n "$INLINE_POLICIES" ]; then
              for POLICY in $INLINE_POLICIES; do
                echo "Deleting inline policy: $POLICY"
                aws iam delete-role-policy --role-name "transinia-prod-ecsTaskRole" --policy-name "$POLICY" || true
                sleep 2
              done
            else
              echo "No inline policies found on task role"
            fi
            
            # Step 4: Delete the role with improved error handling and multiple retries
            echo "Deleting task role..."
            
            # Wait a moment for policy detachments to complete
            sleep 5
            
            MAX_RETRIES=5
            for ((i=1; i<=MAX_RETRIES; i++)); do
              # Check if the role still exists
              if aws iam get-role --role-name "transinia-prod-ecsTaskRole" &>/dev/null; then
                echo "Attempt $i/$MAX_RETRIES: Deleting role 'transinia-prod-ecsTaskRole'..."
                aws iam delete-role --role-name "transinia-prod-ecsTaskRole" && { echo "Role successfully deleted"; break; } || echo "Delete attempt $i/$MAX_RETRIES failed, retrying in 15 seconds..."
                sleep 15
              else
                echo "Role 'transinia-prod-ecsTaskRole' no longer exists, moving on"
                break
              fi
            done
          else
            echo "Task role 'transinia-prod-ecsTaskRole' not found, skipping cleanup"
          fi
          
          # Step 3: Check and delete VPC if it exists
          VPC_EXISTS=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=transinia-prod-vpc" --query "Vpcs[0].VpcId" --output text 2>/dev/null || echo "NOT_FOUND")
          if [ "$VPC_EXISTS" != "NOT_FOUND" ]; then
            echo "Found VPC: $VPC_EXISTS. Starting deletion process..."
            
            # First, get all dependent resources
            SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_EXISTS" --query "Subnets[*].SubnetId" --output text)
            ROUTE_TABLES=$(aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$VPC_EXISTS" --query "RouteTables[*].RouteTableId" --output text)
            SECURITY_GROUPS=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_EXISTS" --query "SecurityGroups[?GroupName != 'default'].GroupId" --output text)
            INTERNET_GATEWAYS=$(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_EXISTS" --query "InternetGateways[*].InternetGatewayId" --output text)
            NAT_GATEWAYS=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_EXISTS" --query "NatGateways[*].NatGatewayId" --output text)
            
            # Delete any load balancers in the VPC
            echo "Checking for load balancers..."
            LOAD_BALANCERS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?VpcId=='$VPC_EXISTS'].LoadBalancerArn" --output text)
            if [ -n "$LOAD_BALANCERS" ]; then
              for LB in $LOAD_BALANCERS; do
                echo "Deleting load balancer: $LB"
                aws elbv2 delete-load-balancer --load-balancer-arn $LB
                # Wait for deletion to complete
                sleep 10
              done
            fi
            
            # Delete ENIs
            echo "Deleting network interfaces..."
            ENIS=$(aws ec2 describe-network-interfaces --filters "Name=vpc-id,Values=$VPC_EXISTS" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text)
            if [ -n "$ENIS" ]; then
              for ENI in $ENIS; do
                echo "Deleting network interface: $ENI"
                aws ec2 delete-network-interface --network-interface-id $ENI || true
                sleep 2
              done
            fi
            
            # Delete NAT gateways
            if [ -n "$NAT_GATEWAYS" ]; then
              for NAT in $NAT_GATEWAYS; do
                echo "Deleting NAT gateway: $NAT"
                aws ec2 delete-nat-gateway --nat-gateway-id $NAT
                # NAT gateway deletion takes time
                sleep 5
              done
              
              # Wait for NAT gateways to be deleted
              echo "Waiting for NAT gateways to be deleted..."
              sleep 30
            fi
            
            # Delete security groups
            if [ -n "$SECURITY_GROUPS" ]; then
              for SG in $SECURITY_GROUPS; do
                echo "Deleting security group: $SG"
                aws ec2 delete-security-group --group-id $SG || true
                sleep 1
              done
            fi
            
            # Detach and delete internet gateways
            if [ -n "$INTERNET_GATEWAYS" ]; then
              for IGW in $INTERNET_GATEWAYS; do
                echo "Detaching and deleting internet gateway: $IGW"
                aws ec2 detach-internet-gateway --internet-gateway-id $IGW --vpc-id $VPC_EXISTS || true
                sleep 2
                aws ec2 delete-internet-gateway --internet-gateway-id $IGW || true
                sleep 1
              done
            fi
            
            # Delete custom route tables
            if [ -n "$ROUTE_TABLES" ]; then
              for RT in $ROUTE_TABLES; do
                # Skip the main route table
                IS_MAIN=$(aws ec2 describe-route-tables --route-table-ids $RT --query "RouteTables[0].Associations[?Main].RouteTableAssociationId" --output text)
                if [ -z "$IS_MAIN" ]; then
                  echo "Deleting route table: $RT"
                  # First, disassociate any subnets
                  ASSOCIATIONS=$(aws ec2 describe-route-tables --route-table-ids $RT --query "RouteTables[0].Associations[?!Main].RouteTableAssociationId" --output text)
                  if [ -n "$ASSOCIATIONS" ]; then
                    for ASSOC in $ASSOCIATIONS; do
                      aws ec2 disassociate-route-table --association-id $ASSOC || true
                      sleep 1
                    done
                  fi
                  # Then delete the route table
                  aws ec2 delete-route-table --route-table-id $RT || true
                  sleep 1
                fi
              done
            fi
            
            # Delete subnets
            if [ -n "$SUBNETS" ]; then
              for SUBNET in $SUBNETS; do
                echo "Deleting subnet: $SUBNET"
                aws ec2 delete-subnet --subnet-id $SUBNET || true
                sleep 1
              done
            fi
            
            # Finally delete the VPC
            echo "Deleting VPC: $VPC_EXISTS"
            aws ec2 delete-vpc --vpc-id $VPC_EXISTS || true
            echo "VPC deletion complete"
          else
            echo "VPC not found, skipping VPC cleanup"
          fi
          
          echo "Infrastructure destruction process completed"
          
          # Generate a summary report
          echo -e "\n----- Resource Cleanup Summary -----"
          echo "✅ CloudWatch Log Groups: /ecs/transinia-prod-backend, /ecs/transinia-prod-frontend"
          echo "✅ IAM Roles: transinia-prod-ecsTaskRole, transinia-prod-ecsTaskExecutionRole"
          echo "✅ VPC and related resources"
          echo "-----------------------------------"
          
          exit 0
          
      # Final cleanup step - always runs even if prior steps fail
      - name: Final Resource Cleanup
        if: always()
        run: |
          echo "Performing final cleanup operations..."
          
          # Ensure log groups can be read
          aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-prod-backend" --region ${{ env.AWS_REGION }} || true
          aws logs describe-log-groups --log-group-name-prefix "/ecs/transinia-prod-frontend" --region ${{ env.AWS_REGION }} || true
          
          # Stop and clean up failed tasks
          echo "Cleaning up failed tasks..."
          FAILED_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status STOPPED --query 'taskArns[*]' --output text || echo "")
          if [[ ! -z "$FAILED_TASKS" ]]; then
            echo "Found failed tasks: $FAILED_TASKS"
            for TASK in $FAILED_TASKS; do
              aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK --region ${{ env.AWS_REGION }} || true
            done
          fi
          
          # Check and clean up any running tasks that aren't part of a service
          echo "Checking for orphaned running tasks..."
          RUNNING_TASKS=$(aws ecs list-tasks --cluster ${{ env.CLUSTER_NAME }} --desired-status RUNNING --query 'taskArns[*]' --output text || echo "")
          if [[ ! -z "$RUNNING_TASKS" ]]; then
            for TASK in $RUNNING_TASKS; do
              TASK_SERVICE=$(aws ecs describe-tasks --cluster ${{ env.CLUSTER_NAME }} --tasks $TASK --query 'tasks[0].group' --output text || echo "")
              if [[ ! $TASK_SERVICE == service:* ]]; then
                echo "Stopping orphaned task: $TASK"
                aws ecs stop-task --cluster ${{ env.CLUSTER_NAME }} --task $TASK --region ${{ env.AWS_REGION }} || true
              fi
            done
          fi

      - name: Deployment Success
        run: echo "Production deployment completed successfully!"